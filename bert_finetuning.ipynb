{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import copy\n",
    "import transformers\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import AdamW, BertTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "# from model import SentimentClassifierWithMultipleHeads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = dataset['train'].to_pandas()\n",
    "# split train into train and validation\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.1, random_state=42)\n",
    "# df_val = dataset['validation'].to_pandas()\n",
    "# split validation into validation and test\n",
    "# df_val, df_test = train_test_split(df_val, test_size=0.5, random_state=42)\n",
    "df_test = dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column_name = 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_messages = df_train[text_column_name].to_list()\n",
    "train_labels = df_train['label'].to_list()\n",
    "val_messages = df_val[text_column_name].to_list()\n",
    "val_labels = df_val['label'].to_list()\n",
    "test_messages = df_test[text_column_name].to_list()\n",
    "test_labels = df_test['label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' Original: ', train_messages[0])\n",
    "\n",
    "# Print the text split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(train_messages[0]))\n",
    "\n",
    "# Print the text mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_messages[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, tokenizer, max_len):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # text to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_len,           # Pad & truncate all texts.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_masks = tokenize_texts(train_messages, tokenizer, max_len)\n",
    "val_inputs, val_masks = tokenize_texts(val_messages, tokenizer, max_len)\n",
    "test_inputs, test_masks = tokenize_texts(test_messages, tokenizer, max_len)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "test_labels = torch.tensor(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all shapes\n",
    "print(\"Train Messages: \", len(train_messages))\n",
    "print(\"Train Inputs: \", train_inputs.shape)\n",
    "print(\"Train Masks: \", train_masks.shape)\n",
    "print(\"Train Labels: \", train_labels.shape)\n",
    "print(\"Validation Messages: \", len(val_messages))\n",
    "print(\"Validation Inputs: \", val_inputs.shape)\n",
    "print(\"Validation Masks: \", val_masks.shape)\n",
    "print(\"Validation Labels: \", val_labels.shape)\n",
    "print(\"Test Messages: \", len(test_messages))\n",
    "print(\"Test Inputs: \", test_inputs.shape)\n",
    "print(\"Test Masks: \", test_masks.shape)\n",
    "print(\"Test Labels: \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SentimentClassifierWithMultipleHeads(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(SentimentClassifierWithMultipleHeads, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.num_labels = num_labels\n",
    "        # 12 heads for BERT\n",
    "        self.classification_heads = [torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(128, self.num_labels)).to(device) for _ in range(12)]\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, output_hidden_states=True, output_attentions=True)\n",
    "        hidden_states = outputs[2]\n",
    "        hidden_states = [torch.mean(layer, dim=1) for layer in hidden_states]\n",
    "        logits = [head(hidden_states[i+1]) for i, head in enumerate(self.classification_heads)]\n",
    "        probs = [torch.nn.functional.softmax(logit, dim=-1) for logit in logits]\n",
    "        loss = [torch.nn.functional.cross_entropy(logit, labels.float()) for logit in logits]\n",
    "        \n",
    "        return loss, logits\n",
    "\n",
    "    def predict(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs[2]\n",
    "        hidden_states = [torch.mean(layer, dim=1) for layer in hidden_states]\n",
    "        \n",
    "        # pass each layer to its own classification head\n",
    "        logits = [head(hidden_states[i+1]) for i, head in enumerate(self.classification_heads)]\n",
    "        \n",
    "        # Take softmax of logits to get probabilities\n",
    "        probs = [torch.nn.functional.softmax(logit, dim=-1) for logit in logits]\n",
    "        \n",
    "        # Get predictions for each head\n",
    "        predictions = [torch.argmax(prob, dim=-1) for prob in probs]\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentClassifierWithMultipleHeads('bert-base-uncased', 2)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def compute_aggregate_and_classwise_metrics(predicted_labels, true_labels):\n",
    "    classification_metrics = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "    return classification_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "all_stats = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    train_dataloader = tqdm(train_dataloader, desc=f\"Epoch {epoch_i + 1}\")\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_labels = torch.nn.functional.one_hot(b_labels, num_classes=2).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids, b_input_mask, b_labels)\n",
    "        loss = outputs[0]\n",
    "        # sum loss of all heads\n",
    "        loss = sum(loss)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "    model.eval()\n",
    "    eval_preds = {i: [] for i in range(12)}\n",
    "    eval_labels = []\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    validation_dataloader = tqdm(validation_dataloader, desc=\"Validation\")\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, b_input_mask, b_labels_one_hot)\n",
    "        loss = outputs[0]\n",
    "        logits = [logit.detach().cpu().numpy() for logit in outputs[1]]  # Detach each tensor in the list\n",
    "        loss = sum(loss)\n",
    "        total_eval_loss += loss.item()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions = [np.argmax(logit, axis=1).flatten() for logit in logits]\n",
    "        for i in range(12):\n",
    "            eval_preds[i].extend(predictions[i])\n",
    "        eval_labels.extend(label_ids)\n",
    "    val_metrics = [compute_aggregate_and_classwise_metrics(eval_preds[i], eval_labels) for i in range(12)]\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(\"Validation Loss: {}\".format(avg_val_loss))\n",
    "    # print validation metrics for each head\n",
    "    for i in range(12):\n",
    "        print(\"Validation Metrics for Head {}: {}\".format(i, val_metrics[i]))\n",
    "\n",
    "    model.eval()\n",
    "    test_preds = {i: [] for i in range(12)}\n",
    "    test_labels = []\n",
    "    total_test_loss = 0\n",
    "    nb_test_steps = 0\n",
    "    test_dataloader = tqdm(test_dataloader, desc=\"Test\")\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, b_input_mask, b_labels_one_hot)\n",
    "        loss = outputs[0]\n",
    "        logits = [logit.detach().cpu().numpy() for logit in outputs[1]]  # Detach each tensor in the list\n",
    "        loss = sum(loss)\n",
    "        total_eval_loss += loss.item()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions = [np.argmax(logit, axis=1).flatten() for logit in logits]\n",
    "        for i in range(12):\n",
    "            test_preds[i].extend(predictions[i])\n",
    "        test_labels.extend(label_ids)\n",
    "    test_metrics = [compute_aggregate_and_classwise_metrics(test_preds[i], test_labels) for i in range(12)]\n",
    "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "    print(\"Test Loss: {}\".format(avg_test_loss))\n",
    "    for i in range(12):\n",
    "        print(\"Test Metrics for Head {}: {}\".format(i, test_metrics[i]))\n",
    "    \n",
    "    all_stats.append({\n",
    "        'epoch': epoch_i + 1,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': avg_val_loss,\n",
    "        'test_loss': avg_test_loss,\n",
    "        'val_metrics': val_metrics,\n",
    "        'test_metrics': test_metrics\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"Training complete!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot validation stats\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set(style='darkgrid')\n",
    "# sns.set(font_scale=1.5)\n",
    "# plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# print macrof1\n",
    "val_macrof1_per_head = {}\n",
    "for i in range(12):\n",
    "    val_macrof1_per_head[i] = [stat['val_metrics'][i]['macro avg']['f1-score'] for stat in all_stats]\n",
    "for i in range(12):\n",
    "    plt.plot(val_macrof1_per_head[i], label='Head {}'.format(i))\n",
    "# x axis is 1,2,3,4 epochs\n",
    "plt.xticks([i for i in range(epochs)])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Macro F1 Score')\n",
    "plt.title('Macro F1 Score Per Head')\n",
    "plt.legend()\n",
    "# increase the size of the plot to make it more readable\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_macrof1_per_head = {}\n",
    "for i in range(12):\n",
    "    test_macrof1_per_head[i] = [stat['test_metrics'][i]['macro avg']['f1-score'] for stat in all_stats]\n",
    "for i in range(12):\n",
    "    plt.plot(test_macrof1_per_head[i], label='Head {}'.format(i))\n",
    "# x axis is 1,2,3,4 epochs\n",
    "plt.xticks([i for i in range(epochs)])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Macro F1 Score')\n",
    "plt.title('Macro F1 Score Per Head')\n",
    "plt.legend()\n",
    "# increase the size of the plot to make it more readable\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
